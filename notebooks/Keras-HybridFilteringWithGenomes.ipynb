{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1beeb0d2-ea99-426f-9ed5-f2b05b46e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 08:03:13.416305: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-13 08:03:13.577684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734073393.639074     465 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734073393.657486     465 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-13 08:03:13.810023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import all relevant libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras import activations\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.optimizers import Adam, AdamW, RMSprop\n",
    "from keras.optimizers.schedules import ExponentialDecay,CosineDecayRestarts\n",
    "from tensorflow.keras import mixed_precision\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ed912b-32ae-4556-b0f6-e5259c0a65e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# checking that GPU is available for processing\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973a37da-f25c-4524-98cb-04f338989dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load relevant databases\n",
    "df_movies = pd.read_csv(\"../data/movies.csv\")\n",
    "df_genometags = pd.read_csv(\"../data/genome-tags.csv\", index_col = 0)\n",
    "df_genome_scores = pd.read_csv(\"../data/genome-scores.csv\")\n",
    "df = pd.read_csv(\"../data/ratings.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e31e978-504e-4e26-addc-174507cb350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 138493, Number of Movies: 10370, Number of Genome Tags: 1127, Min rating: 0.5, Max rating: 5.0\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# only include Movies with genome tags\n",
    "movie_genomes = df_genome_scores.drop_duplicates(subset=[\"movieId\"])\n",
    "movie_genomes = movie_genomes[\"movieId\"]\n",
    "df = df[df[\"movieId\"].isin(movie_genomes)]\n",
    "df_genome_scores = df_genome_scores[df_genome_scores.tagId != 742] #removing the outlier \"original\"\n",
    "\n",
    "# encode users and movies\n",
    "user_ids = df[\"userId\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "movie_ids = df[\"movieId\"].unique().tolist()\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
    "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
    "df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n",
    "df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n",
    "#get lenghts for implementing Keras layers\n",
    "num_users = len(user2user_encoded)\n",
    "num_movies = len(movie_encoded2movie)\n",
    "num_genomes = len(df_genome_scores[\"tagId\"].unique())\n",
    "df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n",
    "# min and max ratings will be used to normalize the ratings later\n",
    "min_rating = min(df[\"rating\"])\n",
    "max_rating = max(df[\"rating\"])\n",
    "\n",
    "# Normalize genome scores \n",
    "\n",
    "df_genome_scores[\"relevance\"] = df_genome_scores[\"relevance\"] / df_genome_scores.groupby(\"movieId\")[\"relevance\"].transform('sum')\n",
    "df_genome_scores = df_genome_scores[[\"movieId\", \"tagId\",  \"relevance\"]]\n",
    "df_genome_scores.sort_values(\"movieId\")\n",
    "# Create a matrix of genome tag relevance scores for each movie\n",
    "# This will give us a matrix of size (num_movies, num_genomes), where each entry is the relevance of a tag for a movie\n",
    "\n",
    "# Initialize an empty matrix with zeros (num_movies x num_genomes)\n",
    "movie_genome_matrix = np.zeros((df_genome_scores[\"movieId\"].nunique(), num_genomes))\n",
    "# Map movieId to row indices\n",
    "movie_to_row = {movie_id: idx for idx, movie_id in enumerate(df_genome_scores[\"movieId\"].unique())}\n",
    "# Map tagId to column indices\n",
    "tag_to_col = {tag_id: idx for idx, tag_id in enumerate(df_genome_scores[\"tagId\"].unique())}\n",
    "# Populate the matrix with relevance scores\n",
    "for _, row in df_genome_scores.iterrows():\n",
    "    movie_idx = movie_to_row[row[\"movieId\"]]\n",
    "    tag_idx = tag_to_col[row[\"tagId\"]]\n",
    "    movie_genome_matrix[movie_idx, tag_idx] = row[\"relevance\"]\n",
    "# Convert the matrix to a DataFrame (optional, for easier inspection)\n",
    "movie_genome_df = pd.DataFrame(movie_genome_matrix, columns=df_genome_scores[\"tagId\"].unique())\n",
    "# Now movie_genome_df contains the relevance scores for each movie, with a column for each tag.\n",
    "# This matrix is of shape (num_movies, 1128) where each row is a movie and each column is a genome tag relevance.\n",
    "\n",
    "print(\n",
    "    \"Number of users: {}, Number of Movies: {}, Number of Genome Tags: {}, Min rating: {}, Max rating: {}\".format(\n",
    "        num_users, num_movies, num_genomes, min_rating, max_rating\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810c22c8-7515-4c98-81a4-798362e92fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency of each rating\n",
    "ratings = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating))  \n",
    "rating_counts = ratings.value_counts()\n",
    "rating_freq = {rating: count / len(df) for rating, count in rating_counts.items()}\n",
    "\n",
    "# Create a weight map (inverse of the frequency)\n",
    "rating_weights = {rating: 1 / freq for rating, freq in rating_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acfaa95-ee61-4054-82e7-98c06c9a33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using batches, due to the extremely large 20M Dataset\n",
    "#Not using batches results in MemoryErrors\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 512 \n",
    "\n",
    "# Prepare the feature matrix x and target vector y\n",
    "x_genomes = np.asarray(movie_genome_df)\n",
    "x = df[[\"user\", \"movie\"]].values  # Features\n",
    "y = ratings.values\n",
    "#x, y = shuffle(x, y, random_state=42)\n",
    "\n",
    "# Randomly split users\n",
    "unique_users = np.unique(x[:, 0])  # x[:, 0] is the user column\n",
    "\n",
    "# Split users into training and testing\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter original dataset based on the split user ids\n",
    "train_mask = np.isin(x[:, 0], train_users)\n",
    "test_mask = np.isin(x[:, 0], test_users)\n",
    "\n",
    "# Create training and testing sets based on user split\n",
    "x_train = x[train_mask]\n",
    "y_train = y[train_mask]\n",
    "x_test = x[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "# Function to yield batches of data for training\n",
    "\n",
    "def data_generator(x_data, y_data, x_genomes, batch_size=BATCH_SIZE):\n",
    "    num_samples = len(x_data)\n",
    "    \n",
    "    while True:  # Loop over the data forever (for Keras)\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            batch_x = x_data[start:end]\n",
    "            batch_y = y_data[start:end]\n",
    "            \n",
    "            # Get movie IDs and use them to index into x_genomes\n",
    "            batch_movie_ids = batch_x[:, 1].astype(int)\n",
    "            batch_genomes = x_genomes[batch_movie_ids]\n",
    "            \n",
    "             # Get the rating for the batch\n",
    "            batch_ratings = batch_y\n",
    "            \n",
    "            # Compute weights for each rating in the batch\n",
    "            batch_weights = np.array([rating_weights.get(rating, 1) for rating in batch_ratings])\n",
    "           \n",
    "            # Separate user, movie, and genome features\n",
    "            batch_user = batch_x[:, 0].astype(np.int32)\n",
    "            batch_movie = batch_x[:, 1].astype(np.int32)\n",
    "            \n",
    "            # Convert to TensorFlow tensors\n",
    "            batch_user = tf.convert_to_tensor(batch_user, dtype=tf.int32)\n",
    "            batch_movie = tf.convert_to_tensor(batch_movie, dtype=tf.int32)\n",
    "            batch_genomes = tf.convert_to_tensor(batch_genomes, dtype=tf.float32)\n",
    "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.float32)\n",
    "\n",
    "            # Yield the batch for Keras\n",
    "            yield (batch_user, batch_movie, batch_genomes), (batch_y, batch_weights)\n",
    "            \n",
    "# Create data generators for training and testing\n",
    "train_generator = data_generator(x_train, y_train, x_genomes, batch_size=BATCH_SIZE)\n",
    "test_generator = data_generator(x_test, y_test, x_genomes, batch_size=BATCH_SIZE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec4b52c6-b364-42ff-b134-7f6766e5b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user input, creating a layer for our user data\n",
    "DROPOUT = 0.2\n",
    "user_input = Input(shape=[1], name=\"User-Input\")\n",
    "user_embedding = Embedding(num_users, 500, name=\"User-Embedding\", embeddings_regularizer=l2(0.01))(user_input)\n",
    "user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "#user_vec = Dropout(DROPOUT)(user_vec)\n",
    "# Create a movie input, correlating the movies to each user\n",
    "movie_input = Input(shape=[1], name=\"Movie-Input\")\n",
    "movie_embedding = Embedding(num_movies, 500, name=\"Movie-Embedding\", embeddings_regularizer=l2(0.01))(movie_input)\n",
    "movie_vec = Flatten(name=\"Flatten-Movies\")(movie_embedding)\n",
    "#movie_vec = Dropout(DROPOUT)(movie_vec)\n",
    "# Create a genome input, futher describing the movies\n",
    "genome_input = Input(shape=[num_genomes], name = \"Genome-Input\")\n",
    "genome_vec = Dense(1127, name=\"Dense-Genome\", activation = \"relu\")(genome_input)\n",
    "#genome_vec = Dropout(DROPOUT)(genome_vec)\n",
    "user_bias = Embedding(num_users, 1)(user_input)\n",
    "user_bias = Flatten()(user_bias)\n",
    "movie_bias = Embedding(num_movies, 1)(movie_input)\n",
    "movie_bias = Flatten()(movie_bias)\n",
    "#concatenate the layers\n",
    "conc = Concatenate()([user_vec, movie_vec, genome_vec])\n",
    "\n",
    "# add fully-connected-layers\n",
    "fc1 = Dense(512, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=l2(0.001))(conc)\n",
    "fc1 = LeakyReLU(alpha=0.1)(fc1)  # Use LeakyReLU instead of ReLU\n",
    "fc1 = Dropout(DROPOUT)(fc1)\n",
    "fc2 = Dense(256, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=l2(0.001))(fc1)\n",
    "fc2 = LeakyReLU(alpha=0.1)(fc2)  # Use LeakyReLU instead of ReLU\n",
    "fc2 = Dropout(DROPOUT)(fc2)\n",
    "fc3 = Dense(128, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=l2(0.001))(fc2)\n",
    "fc3 = LeakyReLU(alpha=0.1)(fc3)  # Use LeakyReLU instead of ReLU\n",
    "fc3 = Dropout(DROPOUT)(fc3)\n",
    "fc4 = Dense(64, activation='relu', kernel_initializer=GlorotUniform(), kernel_regularizer=l2(0.001))(fc3)\n",
    "fc4 = LeakyReLU(alpha=0.1)(fc4)\n",
    "out = Dense(1)(fc4)\n",
    "\n",
    "out_with_biases = (lambda x: x[0] + x[1] + x[2])([out, user_bias, movie_bias])\n",
    "\n",
    "# Create model and compile it\n",
    "model = Model([user_input, movie_input, genome_input], out_with_biases)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "#optimizer = RMSprop(learning_rate=0.0001, rho=0.9, epsilon=1e-07)\n",
    "# Huber loss combines MSE and MAE and is more robust to outliers // unused \n",
    "#loss = tf.keras.losses.Huber(delta=1.0)  \n",
    "optimizer = RMSprop(learning_rate=1e-4, rho=0.9, epsilon=1e-07)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.RootMeanSquaredError()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e015d654-4668-456f-a694-d4f5e18bbaa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 10ms/step - loss: 18.1413 - mean_absolute_error: 0.1806 - mean_squared_error: 0.0535 - root_mean_squared_error: 0.2300 - val_loss: 0.0461 - val_mean_absolute_error: 0.1662 - val_mean_squared_error: 0.0461 - val_root_mean_squared_error: 0.2146 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 10ms/step - loss: 0.0449 - mean_absolute_error: 0.1650 - mean_squared_error: 0.0449 - root_mean_squared_error: 0.2119 - val_loss: 0.0452 - val_mean_absolute_error: 0.1652 - val_mean_squared_error: 0.0452 - val_root_mean_squared_error: 0.2126 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 10ms/step - loss: 0.0440 - mean_absolute_error: 0.1631 - mean_squared_error: 0.0440 - root_mean_squared_error: 0.2099 - val_loss: 0.0450 - val_mean_absolute_error: 0.1649 - val_mean_squared_error: 0.0450 - val_root_mean_squared_error: 0.2120 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 10ms/step - loss: 0.0435 - mean_absolute_error: 0.1618 - mean_squared_error: 0.0435 - root_mean_squared_error: 0.2085 - val_loss: 0.0448 - val_mean_absolute_error: 0.1648 - val_mean_squared_error: 0.0448 - val_root_mean_squared_error: 0.2118 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 10ms/step - loss: 0.0430 - mean_absolute_error: 0.1608 - mean_squared_error: 0.0430 - root_mean_squared_error: 0.2073 - val_loss: 0.0448 - val_mean_absolute_error: 0.1648 - val_mean_squared_error: 0.0448 - val_root_mean_squared_error: 0.2116 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 10ms/step - loss: 0.0426 - mean_absolute_error: 0.1599 - mean_squared_error: 0.0426 - root_mean_squared_error: 0.2064 - val_loss: 0.0448 - val_mean_absolute_error: 0.1647 - val_mean_squared_error: 0.0448 - val_root_mean_squared_error: 0.2115 - learning_rate: 9.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 10ms/step - loss: 0.0423 - mean_absolute_error: 0.1592 - mean_squared_error: 0.0423 - root_mean_squared_error: 0.2056 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 9.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 10ms/step - loss: 0.0420 - mean_absolute_error: 0.1585 - mean_squared_error: 0.0420 - root_mean_squared_error: 0.2048 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 9.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 10ms/step - loss: 0.0417 - mean_absolute_error: 0.1579 - mean_squared_error: 0.0417 - root_mean_squared_error: 0.2042 - val_loss: 0.0447 - val_mean_absolute_error: 0.1644 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 9.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 10ms/step - loss: 0.0414 - mean_absolute_error: 0.1574 - mean_squared_error: 0.0414 - root_mean_squared_error: 0.2036 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 9.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 10ms/step - loss: 0.0412 - mean_absolute_error: 0.1569 - mean_squared_error: 0.0412 - root_mean_squared_error: 0.2031 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2114 - learning_rate: 8.1000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 10ms/step - loss: 0.0410 - mean_absolute_error: 0.1565 - mean_squared_error: 0.0410 - root_mean_squared_error: 0.2026 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2114 - learning_rate: 8.1000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 10ms/step - loss: 0.0409 - mean_absolute_error: 0.1561 - mean_squared_error: 0.0409 - root_mean_squared_error: 0.2021 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2114 - learning_rate: 8.1000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 10ms/step - loss: 0.0407 - mean_absolute_error: 0.1557 - mean_squared_error: 0.0407 - root_mean_squared_error: 0.2017 - val_loss: 0.0447 - val_mean_absolute_error: 0.1644 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 8.1000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 10ms/step - loss: 0.0405 - mean_absolute_error: 0.1553 - mean_squared_error: 0.0405 - root_mean_squared_error: 0.2013 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 8.1000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 10ms/step - loss: 0.0404 - mean_absolute_error: 0.1550 - mean_squared_error: 0.0404 - root_mean_squared_error: 0.2009 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 7.2900e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 11ms/step - loss: 0.0402 - mean_absolute_error: 0.1547 - mean_squared_error: 0.0402 - root_mean_squared_error: 0.2006 - val_loss: 0.0447 - val_mean_absolute_error: 0.1646 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 7.2900e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 12ms/step - loss: 0.0401 - mean_absolute_error: 0.1544 - mean_squared_error: 0.0401 - root_mean_squared_error: 0.2003 - val_loss: 0.0447 - val_mean_absolute_error: 0.1646 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 7.2900e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 12ms/step - loss: 0.0400 - mean_absolute_error: 0.1541 - mean_squared_error: 0.0400 - root_mean_squared_error: 0.2000 - val_loss: 0.0447 - val_mean_absolute_error: 0.1646 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 7.2900e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 12ms/step - loss: 0.0399 - mean_absolute_error: 0.1539 - mean_squared_error: 0.0399 - root_mean_squared_error: 0.1997 - val_loss: 0.0447 - val_mean_absolute_error: 0.1645 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 7.2900e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 12ms/step - loss: 0.0398 - mean_absolute_error: 0.1536 - mean_squared_error: 0.0398 - root_mean_squared_error: 0.1994 - val_loss: 0.0447 - val_mean_absolute_error: 0.1644 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 6.5610e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m30841/30841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 12ms/step - loss: 0.0397 - mean_absolute_error: 0.1535 - mean_squared_error: 0.0397 - root_mean_squared_error: 0.1992 - val_loss: 0.0447 - val_mean_absolute_error: 0.1644 - val_mean_squared_error: 0.0447 - val_root_mean_squared_error: 0.2115 - learning_rate: 6.5610e-05\n"
     ]
    }
   ],
   "source": [
    "# create an EarlyStopping, stopping training if val_loss doesnt improve after 10 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        lr = lr * 0.9  # Decay learning rate by 10% every 5 epochs\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "# Train the model based on the data split\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs=100, \n",
    "    verbose = 1,\n",
    "    callbacks = [early_stopping, lr_scheduler],\n",
    "    validation_data=test_generator,\n",
    "    steps_per_epoch=len(x_train) // BATCH_SIZE, \n",
    "    validation_steps=len(x_test) // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc98f1a-d689-4778-8e50-c5da549e7955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_dir = \"../models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model.save(os.path.join(model_dir, \"movie_recommendation_model_with_genomes_retrained.keras\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e18ec-e54b-48b3-b639-3e60d11f19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genome_df.to_csv(\"../data/movie_genome_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to ensure it saved properly\n",
    "loaded_model = load_model(os.path.join(model_dir, \"movie_recommendation_model_with_genomes0_1614.keras\"))\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01530e6-3027-4f5b-9d0e-cb2148b033c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training and validation loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss - mean absolute error\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "#Plot training and validation accuracy\n",
    "plt.plot(history.history[\"mean_squared_error\"])\n",
    "plt.plot(history.history[\"val_mean_squared_error\"])\n",
    "\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"mean absolute error\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_evaluate(x_data, y_data, x_genomes, batch_size=BATCH_SIZE):\n",
    "    num_samples = len(x_data)\n",
    "    \n",
    "    while True:  # Loop over the data forever (for Keras)\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            batch_x = x_data[start:end]\n",
    "            batch_y = y_data[start:end]\n",
    "            \n",
    "            # Get movie IDs and use them to index into x_genomes\n",
    "            batch_movie_ids = batch_x[:, 1].astype(int)\n",
    "            batch_genomes = x_genomes[batch_movie_ids]\n",
    "            \n",
    "             # Get the rating for the batch\n",
    "            batch_ratings = batch_y\n",
    "            \n",
    "            # Compute weights for each rating in the batch\n",
    "            batch_weights = np.array([rating_weights.get(rating, 1) for rating in batch_ratings])\n",
    "           \n",
    "            # Separate user, movie, and genome features\n",
    "            batch_user = batch_x[:, 0].astype(np.int32)\n",
    "            batch_movie = batch_x[:, 1].astype(np.int32)\n",
    "            \n",
    "            # Convert to TensorFlow tensors\n",
    "            batch_user = tf.convert_to_tensor(batch_user, dtype=tf.int32)\n",
    "            batch_movie = tf.convert_to_tensor(batch_movie, dtype=tf.int32)\n",
    "            batch_genomes = tf.convert_to_tensor(batch_genomes, dtype=tf.float32)\n",
    "            batch_y = tf.convert_to_tensor(batch_y, dtype=tf.float32)\n",
    "\n",
    "            # Yield the batch for Keras\n",
    "            yield (batch_user, batch_movie, batch_genomes), batch_y\n",
    "            \n",
    "            \n",
    "evaluation_generator = data_generator_evaluate(x_test, y_test, x_genomes, batch_size=BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca8334-84f1-4e89-970f-a8518ebacd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.evaluate(evaluation_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
